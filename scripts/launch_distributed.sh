#!/bin/bash

# Usage: sbatch launch.sh <BASE_DIR_TO_R2C_SOURCE>
# Make sure to have run setup_env.sh first to create the environment.

#SBATCH --job-name=r2c_dist
#SBATCH --output=/checkpoint/%u/logs/r2c-%j.out
#SBATCH --error=/checkpoint/%u/logs/r2c-%j.err
#SBATCH --partition=uninterrupted
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:8
#SBATCH --cpus-per-task=32
#SBATCH --mem=200G
#SBATCH --time=24:00:00
#SBATCH --open-mode=append

# TODO (viswanath): nodes=4? change MASTER_ADDR as well

. /usr/share/modules/init/sh

source deactivate

module purge
module load cuda/9.0
module load NCCL/2.2.12-1-cuda.9.0
module load cudnn/v7.0-cuda.9.0
module load anaconda3/5.0.1

source activate /private/home/"$USER"/.conda/envs/vcr

BASEDIR=${2:-"/private/home/$USER/projects/r2c"}
SOURCE="$BASEDIR"/models/train.py
PARAMS="$BASEDIR"/models/multiatt/default.json

CHECKPOINT_DIR=/checkpoint/$USER/r2c/$SLURM_JOB_ID
mkdir -p $CHECKPOINT_DIR

MASTER_ADDR="${SLURM_NODELIST//[}"
export MASTER_ADDR="${MASTER_ADDR%,*}"
export MASTER_PORT=29500
export WORLD_SIZE=${SLURM_NTASKS}

echo "Running distributed job $SLURM_JOB_ID on $SLURM_NNODES nodes: $SLURM_NODELIST"
echo "World Size: $WORLD_SIZE"
echo "Master: $MASTER_ADDR:$MASTER_PORT"
echo "GPUs/node: $CUDA_VISIBLE_DEVICES"
echo "Checkpoint dir: $CHECKPOINT_DIR"

# num_workers > 0 requires using forkserver/spawn in
# multiprocessing.set_start_method to avoid NCCL/GLOO deadlocks
# in distributed setting, but forkserver causes pickle related issues
# with allennlp. Setting num_workers=0 for now in distributed setting.
# TODO (viswanath): May be use multi-process single-GPU setup?
srun --label python $SOURCE --params $PARAMS --folder $CHECKPOINT_DIR --no_tqdm --num_workers 0
